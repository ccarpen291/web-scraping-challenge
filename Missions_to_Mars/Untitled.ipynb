{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pymongo\n",
    "import requests\n",
    "from time import sleep\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    # executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    # return Browser(\"chrome\", **executable_path, headless=False)\n",
    "    return Browser(\"chrome\", headless=True) #headless = true means no browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like much of the rest of the world, the Mars rover team is pushing forward with its mission-critical work while putting the health and safety of their colleagues and community first.\n",
      " \n",
      "How NASA's Perseverance Mars Team Adjusted to Work in the Time of Coronavirus \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL of page to be scraped\n",
    "browser = init_browser()\n",
    "#url = 'https://mars.nasa.gov/news/'\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "browser.visit(url)\n",
    "\n",
    "#content_title was in the original html\n",
    "if browser.is_element_present_by_css('div.content_title', wait_time=3):\n",
    "    #First look for all of the html\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #what we are looking for is in a div, nested deep in a title\n",
    "    results = soup.find_all('div', class_='content_title')\n",
    "    #We do .text becasue we are pulling out the html text deep listed\n",
    "    news_title = results[1].text\n",
    "    #This will return a list for the article teaser body, which comes from the html code\n",
    "    results = soup.find_all('div', class_='article_teaser_body')\n",
    "    #This does not have a nested anchor so we will do results 0 here\n",
    "    news_paragraph = results[0].text\n",
    "    print(news_paragraph)\n",
    "    print(' ')\n",
    "    print(news_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA16153_ip.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url)\n",
    "results = browser.find_by_id('full_image')\n",
    "img = results[0]\n",
    "#This clicks on the button \"full Image\" to take you to the website\n",
    "img.click()\n",
    "sleep(7)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "featured_image_url = soup.find(\"img\", class_=\"fancybox-image\")[\"src\"]\n",
    "base_url = 'https://www.jpl.nasa.gov'\n",
    "full_featured_image_url = (f'{base_url}{featured_image_url}')\n",
    "print(full_featured_image_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to twitter and save the latest tweet for mars weather and save as 'mars_weather'\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "#this visits the url\n",
    "browser.visit(url)\n",
    "\n",
    "#web scraping via splinter\n",
    "\n",
    "#Went to first tweet on the above website, inspect then copy from xpath and selected copy from xpath\n",
    "#this xpath corresponds directly to the URL\n",
    "xpath = '/html/body/div/div/div/div[2]/main/div/div/div/div/div/div/div/div/div[2]/section/div/div/div[1]/div/div/div/article/div/div[2]/div[2]/div[2]/div[1]/div/span'\n",
    "\n",
    "#We found xpath from twitter, navigating to the first tweet, right click and copy full xpath\n",
    "#this activates silenium, with a wait time of 10 seconds to let the page load\n",
    "#This is the line that checks to exist it's real\n",
    "# x paths are helpful to drill down to the item that we need\n",
    "if browser.is_element_present_by_xpath(xpath, wait_time=3):\n",
    "    #Select the html element we copied above\n",
    "    first_tweet = browser.find_by_xpath(xpath)\n",
    "    #if we wanted to click, we could do first_tweet.click()\n",
    "    #this is the html of the first tweet\n",
    "    html = first_tweet.html\n",
    "    #Pass just the html we want to Beautiful Soup to return the result\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    print(soup.prettify())\n",
    "    #Method 2\n",
    "    print(first_tweet.text)\n",
    "    mars_weather = first_tweet.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0                               1\n",
      "0  Equatorial Diameter:                        6,792 km\n",
      "1       Polar Diameter:                        6,752 km\n",
      "2                 Mass:  6.39 × 10^23 kg  (0.11 Earths)\n",
      "3                Moons:       2 (  Phobos  &  Deimos  )\n",
      "4       Orbit Distance:       227,943,824 km  (1.38 AU)\n",
      "5         Orbit Period:            687 days (1.9 years)\n",
      "6  Surface Temperature:                    -87 to -5 °C\n",
      "7         First Record:               2nd millennium BC\n",
      "8          Recorded By:            Egyptian astronomers\n"
     ]
    }
   ],
   "source": [
    "#use pandas to scrape and return mars facts\n",
    "#Pandas only looks for tables when web scraping\n",
    "#It will ignore everything else\n",
    "# URL of page to be scraped\n",
    "url = 'https://space-facts.com/mars/'\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "# Create BeautifulSoup object; parse with 'lxml'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "#This pulls the first table on the webpage, you can see other tables by changing 0,1,2,3 etc\n",
    "mars_facts = pd.read_html(soup.prettify())[0]\n",
    "#save the html file, makes a file on our file on the local computer\n",
    "mars_facts.to_html('mars_facts.html', index=False, header=False)\n",
    "print(mars_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerberus link is\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg\n",
      "Cerberus title is \n",
      "Cerberus Hemisphere Enhanced\n",
      "Link is\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg\n",
      "title is\n",
      "Schiaparelli Hemisphere Enhanced\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg\n",
      "Syrtis Major Hemisphere Enhanced\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg\n",
      "Valles Marineris Hemisphere Enhanced\n"
     ]
    }
   ],
   "source": [
    "#want to get high resolution of the images from mars and save them down\n",
    "\n",
    "#Goal is to save image url string\n",
    "#Save the hemisphere title\n",
    "#save in a dictionary, using keys 'img_url' and 'title'\n",
    "\n",
    "browser = init_browser()\n",
    "\n",
    "#Url that we want to scrape\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "#Visit the url\n",
    "browser.visit(url)\n",
    "time.sleep(5)\n",
    "\n",
    "#This is the raw html code\n",
    "html = browser.html\n",
    "#putting it in soup so we can better parse it out\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#Find the first link that we are looking to click This will return a list\n",
    "#Find the text that we want to click, this will come from the html and we know it's a text becasue its listed as a text in the html file\n",
    "results = browser.find_by_text('Cerberus Hemisphere Enhanced')\n",
    "#print(results)\n",
    "#Not necessary but can be saved, second method\n",
    "#img = results[0]\n",
    "#this is correct, second method\n",
    "#img.click()\n",
    "\n",
    "#This advances to the next webpage\n",
    "results.click()\n",
    "time.sleep(5)\n",
    "#Grab the html on the next webpage\n",
    "html = browser.html\n",
    "#Pass to the soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#Find the correct html code\n",
    "#Find all returns everything in a list\n",
    "#instead find gets the first one\n",
    "results = soup.find_all('div', class_='downloads')\n",
    "#Further cull down the anchor of the results\n",
    "cerberus_link = results[0].find('li').a['href']\n",
    "print(\"Cerberus link is\")\n",
    "print(cerberus_link)\n",
    "\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "#When you use \"find\" that gets you a non list\n",
    "#When you use find all that gets you a list of everything tha tis found so you will need to do do title[0]\n",
    "cerberus_title = soup.find('h2', class_='title').text\n",
    "print(\"Cerberus title is \")\n",
    "print(cerberus_title)\n",
    "\n",
    "#Create the dictionary to store the information\n",
    "cerberus={\"title\":cerberus_title,\"image_url\":cerberus_link}\n",
    "\n",
    "#Url that we want to scrape\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "#Visit the url\n",
    "browser.visit(url)\n",
    "time.sleep(5)\n",
    "#This is the raw html code\n",
    "html = browser.html\n",
    "#putting it in soup so we can better parse it out\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#Find the first link that we are looking to click This will return a list\n",
    "#Find the text that we want to click, this will come from the html and we know it's a text becasue its listed as a text in the html file\n",
    "Schiaparelli_results = browser.find_by_text('Schiaparelli Hemisphere Enhanced')\n",
    "\n",
    "Schiaparelli_results.click()\n",
    "time.sleep(5)\n",
    "#Grab the html on the next webpage\n",
    "html = browser.html\n",
    "#Pass to the soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#Find the correct html code\n",
    "#Find all returns everything in a list\n",
    "#instead find gets the first one\n",
    "results = soup.find_all('div', class_='downloads')\n",
    "#Further cull down the anchor of the results\n",
    "Schiaparelli_link = results[0].find('li').a['href']\n",
    "print(\"Link is\")\n",
    "print(Schiaparelli_link)\n",
    "\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "#When you use \"find\" that gets you a non list\n",
    "#When you use find all that gets you a list of everything tha tis found so you will need to do do title[0]\n",
    "Schiaparelli_title = soup.find('h2', class_='title').text\n",
    "print(\"title is\")\n",
    "print(Schiaparelli_title)\n",
    "\n",
    "schiaparelli={\"title\":Schiaparelli_title,\"image_url\":Schiaparelli_link}\n",
    "\n",
    "\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "#Visit the url\n",
    "browser.visit(url)\n",
    "time.sleep(5)\n",
    "#This is the raw html code\n",
    "html = browser.html\n",
    "#putting it in soup so we can better parse it out\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#Find the first link that we are looking to click This will return a list\n",
    "#Find the text that we want to click, this will come from the html and we know it's a text becasue its listed as a text in the html file\n",
    "Syrtis_results = browser.find_by_text('Syrtis Major Hemisphere Enhanced')\n",
    "\n",
    "\n",
    "Syrtis_results.click()\n",
    "time.sleep(5)\n",
    "#Grab the html on the next webpage\n",
    "html = browser.html\n",
    "#Pass to the soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#Find the correct html code\n",
    "#Find all returns everything in a list\n",
    "#instead find gets the first one\n",
    "results = soup.find_all('div', class_='downloads')\n",
    "#Further cull down the anchor of the results\n",
    "Syrtis_link = results[0].find('li').a['href']\n",
    "print(Syrtis_link)\n",
    "\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "#When you use \"find\" that gets you a non list\n",
    "#When you use find all that gets you a list of everything tha tis found so you will need to do do title[0]\n",
    "Syrtis_title = soup.find('h2', class_='title').text\n",
    "print(Syrtis_title)\n",
    "\n",
    "syrtis={\"title\":Syrtis_title,\"image_url\":Syrtis_link}\n",
    "\n",
    "\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "#Visit the url\n",
    "browser.visit(url)\n",
    "time.sleep(5)\n",
    "#This is the raw html code\n",
    "html = browser.html\n",
    "#putting it in soup so we can better parse it out\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#Find the first link that we are looking to click This will return a list\n",
    "#Find the text that we want to click, this will come from the html and we know it's a text becasue its listed as a text in the html file\n",
    "Valles_results = browser.find_by_text('Valles Marineris Hemisphere Enhanced')\n",
    "\n",
    "Valles_results.click()\n",
    "time.sleep(5)\n",
    "#Grab the html on the next webpage\n",
    "html = browser.html\n",
    "#Pass to the soup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#Find the correct html code\n",
    "#Find all returns everything in a list\n",
    "#instead find gets the first one\n",
    "results = soup.find_all('div', class_='downloads')\n",
    "#Further cull down the anchor of the results\n",
    "Valles_link = results[0].find('li').a['href']\n",
    "print(Valles_link)\n",
    "\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "#When you use \"find\" that gets you a non list\n",
    "#When you use find all that gets you a list of everything tha tis found so you will need to do do title[0]\n",
    "Valles_title = soup.find('h2', class_='title').text\n",
    "print(Valles_title)\n",
    "\n",
    "valles={\"title\":Valles_title,\"image_url\":Valles_link}\n",
    "hemisphere_image_urls=[cerberus,schiaparelli,syrtis,valles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Cerberus Hemisphere Enhanced', 'image_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'}, {'title': 'Schiaparelli Hemisphere Enhanced', 'image_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'}, {'title': 'Syrtis Major Hemisphere Enhanced', 'image_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'}, {'title': 'Valles Marineris Hemisphere Enhanced', 'image_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]\n"
     ]
    }
   ],
   "source": [
    "print(hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = soup.find_all('Cerberus Hemisphere Enhanced')\n",
    "# print(results)\n",
    "#img = results[0]\n",
    "\n",
    "\n",
    "#This clicks on the button \"full Image\" to take you to the website\n",
    "# img.click()\n",
    "# sleep(7)\n",
    "# html = browser.html\n",
    "# soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "# featured_image_url = soup.find(\"img\", class_=\"fancybox-image\")[\"src\"]\n",
    "# base_url = 'https://www.jpl.nasa.gov'\n",
    "# full_featured_image_url = (f'{base_url}{featured_image_url}')\n",
    "# print(full_featured_image_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Retrieve page with the requests module\n",
    "#     response = requests.get(url)\n",
    "#     print(response.text)\n",
    "\n",
    "#     # Create BeautifulSoup object; parse with 'html.parser'\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     # Examine the results, then determine element that contains sought info\n",
    "#     # This is what we can take to visual studio and paste it in\n",
    "#     print(soup.prettify())\n",
    "\n",
    "\n",
    "#     browser = init_browser()\n",
    "#     listings = {}\n",
    "\n",
    "#     url = \"https://mars.nasa.gov/news\"\n",
    "#     browser.visit(url)\n",
    "\n",
    "#     html = browser.html\n",
    "    \n",
    "#     #soup = BeautifulSoup(html, \"html.parser\")\n",
    "#     soup = bs(html, \"lxml\")\n",
    "\n",
    "#     # Create BeautifulSoup object; parse with 'lxml'\n",
    "#     #soup = BeautifulSoup(response.text, 'lxml')\n",
    "#     print(soup.prettify())\n",
    "    \n",
    "#     results = soup.find_all('li', class_=)\n",
    "    \n",
    "#     listings[\"headline\"] = soup.find(\"a\", class_=\"result-title\").get_text()\n",
    "#     listings[\"price\"] = soup.find(\"span\", class_=\"result-price\").get_text()\n",
    "#     listings[\"hood\"] = soup.find(\"span\", class_=\"result-hood\").get_text()\n",
    "\n",
    "#     return listings\n",
    "\n",
    "\n",
    "\n",
    "#     # Get the average temps\n",
    "#     avg_temps = soup.find('div', id='weather')\n",
    "\n",
    "#     # Get the min avg temp\n",
    "#     min_temp = avg_temps.find_all('strong')[0].text\n",
    "\n",
    "#     # Get the max avg temp\n",
    "#     max_temp = avg_temps.find_all('strong')[1].text\n",
    "\n",
    "#     # BONUS: Find the src for the sloth image\n",
    "#     relative_image_path = soup.find_all('img')[2][\"src\"]\n",
    "#     sloth_img = url + relative_image_path\n",
    "\n",
    "#     # Store data in a dictionary\n",
    "#     costa_data = {\n",
    "#         \"sloth_img\": sloth_img,\n",
    "#         \"min_temp\": min_temp,\n",
    "#         \"max_temp\": max_temp\n",
    "#     }\n",
    "\n",
    "#     # Close the browser after scraping\n",
    "#     browser.quit()\n",
    "\n",
    "#     # Return results\n",
    "#     return costa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
